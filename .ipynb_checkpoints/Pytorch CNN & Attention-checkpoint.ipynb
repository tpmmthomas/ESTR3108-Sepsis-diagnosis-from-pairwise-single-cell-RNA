{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# for reading and displaying images\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "#Transformers\n",
    "from module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data set (small amount to test if it works first)\n",
    "\n",
    "xtrain = r\"data/training_sample_NoSparse.csv.gz\"\n",
    "ytrain = r\"data/training_label_NoSparse.csv.gz\"\n",
    "xtest = r\"data/testing_sample_NoSparse.csv.gz\"\n",
    "ytest = r\"data/testing_label_NoSparse.csv.gz\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15161, 14094)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check balance\n",
    "samplesdf = pd.DataFrame()\n",
    "for df in  pd.read_csv(ytrain,compression =\"gzip\",delimiter=',', chunksize = 10000, header=0):\n",
    "    samplesdf = samplesdf.append(df)\n",
    "y_train = samplesdf.to_numpy()\n",
    "\n",
    "num0 = 0\n",
    "num1 = 0\n",
    "for x in y_train:\n",
    "    if x == 0:\n",
    "        num0 = num0 + 1\n",
    "    else:\n",
    "        num1 = num1 + 1\n",
    "num0,num1 #checking if it is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   ... 3.11 0.   3.69]\n",
      " [0.   0.   0.   ... 3.16 0.   3.33]\n",
      " [0.   0.   0.   ... 2.98 0.   3.64]\n",
      " ...\n",
      " [1.39 0.   0.   ... 1.95 0.   3.57]\n",
      " [0.   2.08 0.   ... 2.71 0.   3.09]\n",
      " [0.   0.   0.   ... 3.12 0.   2.85]]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(xtrain,compression =\"gzip\",delimiter=',', nrows=123, header=0)\n",
    "x_train = df.to_numpy()\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self,samples,labels,numrows):\n",
    "\n",
    "        self.data = pd.read_csv(samples,compression =\"gzip\",delimiter=',', nrows = numrows, header=0)\n",
    "        self.label = pd.read_csv(labels,compression =\"gzip\",delimiter=',', nrows = numrows, header=0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        rna = self.data.iloc[idx]\n",
    "        seplb = self.label.iloc[idx]\n",
    "        rna = np.array([rna])\n",
    "        seplb = np.array([seplb])\n",
    "        rna = rna.astype('double').reshape(-1,3273)\n",
    "        i = 0\n",
    "        d = np.zeros((len(seplb),2))\n",
    "        for x in seplb:\n",
    "            if x == 0:\n",
    "                d[i] = [1,0]\n",
    "            else:\n",
    "                d[i] = [0,1]\n",
    "        sample = {'rna': rna, 'label': d}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(samples=xtrain,labels=ytrain,numrows = 29255)\n",
    "test_dataset = Dataset(samples = xtest,labels = ytest,numrows = 3241)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "EPOCH = 10             # train the training data n times, to save time, we just train 1 epoch\n",
    "LR = 0.0001              # learning rate\n",
    "batch_size = 32\n",
    "wd = LR / EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (x,1, 8949)\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=4,            # n_filters\n",
    "                kernel_size=3,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=1,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (x,64,8949)\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),                      # activation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (x,64, 8949)\n",
    "            nn.Conv1d(4,4,3,1,1),            \n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.MaxPool1d(kernel_size =2, stride=2,ceil_mode = True),                # output shape (x,64,4478)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         # input shape (x,64,4478)\n",
    "            nn.Conv1d(4, 4, 3, 1, 1),     # output shape (x,128,4478)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool1d(kernel_size =2, stride=2,ceil_mode = True),                # output shape (x,128,2238)\n",
    "        )\n",
    "        self.out = nn.Linear(3276 , 2)   # fully connected layer, output 10 classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output = self.out(x)\n",
    "        return output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnn = CNN()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.CrossEntropyLoss()                       # the target label is not one-hotted\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    loss_func = loss_func.cuda()\n",
    "cnn = cnn.double()    \n",
    "print(cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetTransformer(nn.Module):\n",
    "    def __init__(self, dim_input, num_outputs, dim_output,\n",
    "            num_inds=32, dim_hidden=128, num_heads=2, ln=False):\n",
    "        super(SetTransformer, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "                ISAB(dim_input, dim_hidden, num_heads, num_inds, ln=ln),\n",
    "                ISAB(dim_hidden, dim_hidden, num_heads, num_inds, ln=ln))\n",
    "        self.dec = nn.Sequential(\n",
    "                PMA(dim_hidden, num_heads, num_outputs, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "                SAB(dim_hidden, dim_hidden, num_heads, ln=ln),\n",
    "                nn.Linear(dim_hidden, dim_output))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dec(self.enc(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SetTransformer(\n",
      "  (enc): Sequential(\n",
      "    (0): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=3273, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=3273, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=3273, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ISAB(\n",
      "      (mab0): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (mab1): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dec): Sequential(\n",
      "    (0): PMA(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (1): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (2): SAB(\n",
      "      (mab): MAB(\n",
      "        (fc_q): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_k): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_v): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (fc_o): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (3): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "trans = SetTransformer(3273,2,1)\n",
    "optimizer = torch.optim.Adam(trans.parameters(), lr=LR)   \n",
    "loss_func = nn.L1Loss()                      \n",
    "if torch.cuda.is_available():\n",
    "    loss_func = loss_func.cuda()\n",
    "    trans = trans.cuda()\n",
    "trans = trans.double()\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Trans(nn.Module):\n",
    "    def __init__(self, dim_input, num_outputs, dim_output,\n",
    "            num_inds=32, dim_hidden=128, num_heads=4, ln=False):\n",
    "        super(CNN_Trans, self).__init__()\n",
    "        self.conv1 = nn.Sequential(         # input shape (x,1, 8949)\n",
    "            nn.Conv1d(\n",
    "                in_channels=1,              # input height\n",
    "                out_channels=4,            # n_filters\n",
    "                kernel_size=3,              # filter size\n",
    "                stride=1,                   # filter movement/step\n",
    "                padding=1,                  # if want same width and length of this image after con2d, padding=(kernel_size-1)/2 if stride=1\n",
    "            ),                              # output shape (x,64,8949)\n",
    "            nn.BatchNorm1d(4),\n",
    "            nn.ReLU(),                      # activation\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         # input shape (x,64, 8949)\n",
    "            nn.Conv1d(4,4,3,1,1),            \n",
    "            nn.ReLU(),  \n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.MaxPool1d(kernel_size =2, stride=2,ceil_mode = True),                # output shape (x,64,4478)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(         # input shape (x,64,4478)\n",
    "            nn.Conv1d(4, 4, 3, 1, 1),     # output shape (x,128,4478)\n",
    "            nn.ReLU(),                      # activation\n",
    "            nn.MaxPool1d(kernel_size =2, stride=2,ceil_mode = True),                # output shape (x,128,2238)\n",
    "        )\n",
    "        self.enc = nn.Sequential(\n",
    "            SAB(dim_in=1, dim_out=64, num_heads=4),\n",
    "            SAB(dim_in=64, dim_out=64, num_heads=4),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim=64, num_heads=4, num_seeds=1),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = x.reshape(-1,3276,1)\n",
    "        return self.dec(self.enc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cnntrans = CNN_Trans(3276,1,2)\n",
    "optimizer = torch.optim.Adam(cnntrans.parameters(), lr=LR)   \n",
    "loss_func = nn.L1Loss()                      \n",
    "if torch.cuda.is_available():\n",
    "    loss_func = loss_func.cuda()\n",
    "    cnntrans = cnntrans.cuda()\n",
    "cnntrans = cnntrans.double()\n",
    "print(cnntrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallSetTransformer(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            SAB(dim_in=1, dim_out=64, num_heads=4),\n",
    "            SAB(dim_in=64, dim_out=64, num_heads=4),\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            PMA(dim=64, num_heads=4, num_seeds=1),\n",
    "            nn.Linear(in_features=64, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.dec(x)\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "smalltrans = SmallSetTransformer()\n",
    "optimizer = torch.optim.Adam(smalltrans.parameters(), lr=LR)   \n",
    "loss_func = nn.L1Loss()                      \n",
    "if torch.cuda.is_available():\n",
    "    loss_func = loss_func.cuda()\n",
    "    smalltrans = smalltrans.cuda()\n",
    "smalltrans = smalltrans.double()\n",
    "print(smalltrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in train_loader:\n",
    "            rna,labels = batch[\"rna\"], batch[\"label\"] \n",
    "            if torch.cuda.is_available():\n",
    "                rna = rna.cuda()\n",
    "                labels = labels.cuda()\n",
    "            labels = labels.reshape(-1,2)\n",
    "            outputs = model(rna)\n",
    "            outputs = outputs.reshape(-1,2)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "           # print(total,correct)\n",
    "\n",
    "        print('Train Accuracy of the model on the train rna: {} %'.format((correct / total) * 100))\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in test_loader:\n",
    "            rna,labels = batch[\"rna\"], batch[\"label\"] \n",
    "            if torch.cuda.is_available():\n",
    "                rna = rna.cuda()\n",
    "                labels = labels.cuda()\n",
    "            labels = labels.reshape(-1,2)\n",
    "            outputs = model(rna)\n",
    "            outputs = outputs.reshape(-1,2)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    #print(total,correct)\n",
    "\n",
    "        print('Test Accuracy of the model on the test rna: {} %'.format((correct / total) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def train(model): \n",
    "    total_step = len(train_loader)\n",
    "    num_epoch = EPOCH\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            # Run the forward pass\n",
    "            rna,labels = batch[\"rna\"], batch[\"label\"] \n",
    "            if torch.cuda.is_available():\n",
    "                rna = rna.cuda()\n",
    "                labels = labels.cuda()\n",
    "            outputs = model(rna)\n",
    "            labels = labels.long()\n",
    "            #print(outputs)\n",
    "            outputs = outputs.reshape(-1,2)\n",
    "            labels = labels.reshape(-1,2)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # Backprop and perform Adam optimisation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track the accuracy\n",
    "            total = labels.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            _, labels = torch.max(labels.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            train_acc.append(correct / total)\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                        .format(epoch + 1, num_epoch , i + 1, total_step, loss.item(),\n",
    "                                (correct / total) * 100))\n",
    "        test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/915], Loss: 0.5010, Accuracy: 37.50%\n",
      "Epoch [1/10], Step [101/915], Loss: 0.4996, Accuracy: 43.75%\n",
      "Epoch [1/10], Step [201/915], Loss: 0.3276, Accuracy: 81.25%\n",
      "Epoch [1/10], Step [301/915], Loss: 0.3746, Accuracy: 59.38%\n",
      "Epoch [1/10], Step [401/915], Loss: 0.2964, Accuracy: 71.88%\n",
      "Epoch [1/10], Step [501/915], Loss: 0.2813, Accuracy: 75.00%\n",
      "Epoch [1/10], Step [601/915], Loss: 0.2724, Accuracy: 75.00%\n",
      "Epoch [1/10], Step [701/915], Loss: 0.1087, Accuracy: 90.62%\n",
      "Epoch [1/10], Step [801/915], Loss: 0.2145, Accuracy: 78.12%\n",
      "Epoch [1/10], Step [901/915], Loss: 0.3094, Accuracy: 68.75%\n",
      "Train Accuracy of the model on the train rna: 76.44163390873355 %\n",
      "Test Accuracy of the model on the test rna: 73.74267201481024 %\n",
      "Epoch [2/10], Step [1/915], Loss: 0.4201, Accuracy: 59.38%\n",
      "Epoch [2/10], Step [101/915], Loss: 0.1594, Accuracy: 84.38%\n",
      "Epoch [2/10], Step [201/915], Loss: 0.2341, Accuracy: 78.12%\n",
      "Epoch [2/10], Step [301/915], Loss: 0.2023, Accuracy: 81.25%\n",
      "Epoch [2/10], Step [401/915], Loss: 0.3204, Accuracy: 68.75%\n",
      "Epoch [2/10], Step [501/915], Loss: 0.1743, Accuracy: 84.38%\n",
      "Epoch [2/10], Step [601/915], Loss: 0.2018, Accuracy: 84.38%\n",
      "Epoch [2/10], Step [701/915], Loss: 0.1431, Accuracy: 87.50%\n",
      "Epoch [2/10], Step [801/915], Loss: 0.1622, Accuracy: 84.38%\n",
      "Epoch [2/10], Step [901/915], Loss: 0.3308, Accuracy: 65.62%\n",
      "Train Accuracy of the model on the train rna: 84.45393949752179 %\n",
      "Test Accuracy of the model on the test rna: 80.2838630052453 %\n",
      "Epoch [3/10], Step [1/915], Loss: 0.2082, Accuracy: 81.25%\n",
      "Epoch [3/10], Step [101/915], Loss: 0.1240, Accuracy: 90.62%\n",
      "Epoch [3/10], Step [201/915], Loss: 0.1065, Accuracy: 90.62%\n",
      "Epoch [3/10], Step [301/915], Loss: 0.1549, Accuracy: 84.38%\n",
      "Epoch [3/10], Step [401/915], Loss: 0.1343, Accuracy: 87.50%\n",
      "Epoch [3/10], Step [501/915], Loss: 0.1672, Accuracy: 81.25%\n",
      "Epoch [3/10], Step [601/915], Loss: 0.1496, Accuracy: 87.50%\n",
      "Epoch [3/10], Step [701/915], Loss: 0.1381, Accuracy: 87.50%\n",
      "Epoch [3/10], Step [801/915], Loss: 0.1297, Accuracy: 90.62%\n",
      "Epoch [3/10], Step [901/915], Loss: 0.2422, Accuracy: 75.00%\n",
      "Train Accuracy of the model on the train rna: 86.85352931122885 %\n",
      "Test Accuracy of the model on the test rna: 80.99352051835854 %\n",
      "Epoch [4/10], Step [1/915], Loss: 0.0847, Accuracy: 93.75%\n",
      "Epoch [4/10], Step [101/915], Loss: 0.0533, Accuracy: 96.88%\n",
      "Epoch [4/10], Step [201/915], Loss: 0.2970, Accuracy: 71.88%\n",
      "Epoch [4/10], Step [301/915], Loss: 0.1723, Accuracy: 84.38%\n",
      "Epoch [4/10], Step [401/915], Loss: 0.1626, Accuracy: 84.38%\n",
      "Epoch [4/10], Step [501/915], Loss: 0.1467, Accuracy: 84.38%\n",
      "Epoch [4/10], Step [601/915], Loss: 0.1116, Accuracy: 90.62%\n",
      "Epoch [4/10], Step [701/915], Loss: 0.1938, Accuracy: 81.25%\n",
      "Epoch [4/10], Step [801/915], Loss: 0.1585, Accuracy: 84.38%\n",
      "Epoch [4/10], Step [901/915], Loss: 0.0772, Accuracy: 93.75%\n",
      "Train Accuracy of the model on the train rna: 89.68381473252435 %\n",
      "Test Accuracy of the model on the test rna: 83.92471459426103 %\n",
      "Epoch [5/10], Step [1/915], Loss: 0.1591, Accuracy: 84.38%\n",
      "Epoch [5/10], Step [101/915], Loss: 0.1583, Accuracy: 84.38%\n",
      "Epoch [5/10], Step [201/915], Loss: 0.1090, Accuracy: 90.62%\n",
      "Epoch [5/10], Step [301/915], Loss: 0.1144, Accuracy: 90.62%\n",
      "Epoch [5/10], Step [401/915], Loss: 0.1719, Accuracy: 84.38%\n",
      "Epoch [5/10], Step [501/915], Loss: 0.0950, Accuracy: 90.62%\n",
      "Epoch [5/10], Step [601/915], Loss: 0.0789, Accuracy: 93.75%\n",
      "Epoch [5/10], Step [701/915], Loss: 0.0706, Accuracy: 96.88%\n",
      "Epoch [5/10], Step [801/915], Loss: 0.1086, Accuracy: 90.62%\n",
      "Epoch [5/10], Step [901/915], Loss: 0.0770, Accuracy: 90.62%\n",
      "Train Accuracy of the model on the train rna: 91.05110237566228 %\n",
      "Test Accuracy of the model on the test rna: 84.9429188522061 %\n",
      "Epoch [6/10], Step [1/915], Loss: 0.1583, Accuracy: 84.38%\n",
      "Epoch [6/10], Step [101/915], Loss: 0.1599, Accuracy: 84.38%\n",
      "Epoch [6/10], Step [201/915], Loss: 0.1225, Accuracy: 90.62%\n",
      "Epoch [6/10], Step [301/915], Loss: 0.0194, Accuracy: 100.00%\n",
      "Epoch [6/10], Step [401/915], Loss: 0.1279, Accuracy: 90.62%\n",
      "Epoch [6/10], Step [501/915], Loss: 0.0670, Accuracy: 96.88%\n",
      "Epoch [6/10], Step [601/915], Loss: 0.1731, Accuracy: 84.38%\n",
      "Epoch [6/10], Step [701/915], Loss: 0.1953, Accuracy: 81.25%\n",
      "Epoch [6/10], Step [801/915], Loss: 0.1296, Accuracy: 87.50%\n",
      "Epoch [6/10], Step [901/915], Loss: 0.1297, Accuracy: 87.50%\n",
      "Train Accuracy of the model on the train rna: 91.31430524696633 %\n",
      "Test Accuracy of the model on the test rna: 84.54180808392472 %\n",
      "Epoch [7/10], Step [1/915], Loss: 0.0403, Accuracy: 96.88%\n",
      "Epoch [7/10], Step [101/915], Loss: 0.0694, Accuracy: 93.75%\n",
      "Epoch [7/10], Step [201/915], Loss: 0.0348, Accuracy: 96.88%\n",
      "Epoch [7/10], Step [301/915], Loss: 0.1972, Accuracy: 81.25%\n",
      "Epoch [7/10], Step [401/915], Loss: 0.1461, Accuracy: 87.50%\n",
      "Epoch [7/10], Step [501/915], Loss: 0.1483, Accuracy: 84.38%\n",
      "Epoch [7/10], Step [601/915], Loss: 0.0801, Accuracy: 93.75%\n",
      "Epoch [7/10], Step [701/915], Loss: 0.0485, Accuracy: 96.88%\n",
      "Epoch [7/10], Step [801/915], Loss: 0.0478, Accuracy: 96.88%\n",
      "Epoch [7/10], Step [901/915], Loss: 0.1898, Accuracy: 81.25%\n",
      "Train Accuracy of the model on the train rna: 90.97248333618185 %\n",
      "Test Accuracy of the model on the test rna: 83.9864239432274 %\n",
      "Epoch [8/10], Step [1/915], Loss: 0.1440, Accuracy: 84.38%\n",
      "Epoch [8/10], Step [101/915], Loss: 0.0410, Accuracy: 96.88%\n",
      "Epoch [8/10], Step [201/915], Loss: 0.1015, Accuracy: 90.62%\n",
      "Epoch [8/10], Step [301/915], Loss: 0.0894, Accuracy: 93.75%\n",
      "Epoch [8/10], Step [401/915], Loss: 0.1104, Accuracy: 87.50%\n",
      "Epoch [8/10], Step [501/915], Loss: 0.0653, Accuracy: 93.75%\n",
      "Epoch [8/10], Step [601/915], Loss: 0.1435, Accuracy: 87.50%\n",
      "Epoch [8/10], Step [701/915], Loss: 0.0748, Accuracy: 93.75%\n",
      "Epoch [8/10], Step [801/915], Loss: 0.1516, Accuracy: 84.38%\n",
      "Epoch [8/10], Step [901/915], Loss: 0.0913, Accuracy: 90.62%\n",
      "Train Accuracy of the model on the train rna: 92.80123055887883 %\n",
      "Test Accuracy of the model on the test rna: 84.9429188522061 %\n",
      "Epoch [9/10], Step [1/915], Loss: 0.0099, Accuracy: 100.00%\n",
      "Epoch [9/10], Step [101/915], Loss: 0.0534, Accuracy: 93.75%\n",
      "Epoch [9/10], Step [201/915], Loss: 0.1010, Accuracy: 90.62%\n",
      "Epoch [9/10], Step [301/915], Loss: 0.1296, Accuracy: 87.50%\n",
      "Epoch [9/10], Step [401/915], Loss: 0.1640, Accuracy: 87.50%\n",
      "Epoch [9/10], Step [501/915], Loss: 0.0632, Accuracy: 96.88%\n",
      "Epoch [9/10], Step [601/915], Loss: 0.1363, Accuracy: 87.50%\n",
      "Epoch [9/10], Step [701/915], Loss: 0.0748, Accuracy: 93.75%\n",
      "Epoch [9/10], Step [801/915], Loss: 0.2304, Accuracy: 78.12%\n",
      "Epoch [9/10], Step [901/915], Loss: 0.0975, Accuracy: 90.62%\n",
      "Train Accuracy of the model on the train rna: 92.39104426593745 %\n",
      "Test Accuracy of the model on the test rna: 84.23326133909286 %\n",
      "Epoch [10/10], Step [1/915], Loss: 0.1067, Accuracy: 90.62%\n",
      "Epoch [10/10], Step [101/915], Loss: 0.1671, Accuracy: 84.38%\n",
      "Epoch [10/10], Step [201/915], Loss: 0.1577, Accuracy: 84.38%\n",
      "Epoch [10/10], Step [301/915], Loss: 0.1766, Accuracy: 84.38%\n",
      "Epoch [10/10], Step [401/915], Loss: 0.1017, Accuracy: 90.62%\n",
      "Epoch [10/10], Step [501/915], Loss: 0.1011, Accuracy: 90.62%\n",
      "Epoch [10/10], Step [601/915], Loss: 0.0067, Accuracy: 100.00%\n",
      "Epoch [10/10], Step [701/915], Loss: 0.0917, Accuracy: 90.62%\n",
      "Epoch [10/10], Step [801/915], Loss: 0.0358, Accuracy: 96.88%\n",
      "Epoch [10/10], Step [901/915], Loss: 0.0411, Accuracy: 96.88%\n",
      "Train Accuracy of the model on the train rna: 92.4935908391728 %\n",
      "Test Accuracy of the model on the test rna: 84.66522678185746 %\n"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "train(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(train_acc, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(train_acc[200:], label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
